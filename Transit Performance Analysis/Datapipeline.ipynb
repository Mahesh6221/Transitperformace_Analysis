{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81686512-b136-4acf-8d56-933b69faf408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Part 1: Load and Explore Data\n",
    "\n",
    "# Read the JSON data into your environment\n",
    "df = spark.read.option(\"multiline\", \"true\").json(\"/Volumes/workspace/default/project_transit_performance_analysis/Swift Assignment 4 - Dataset (1).json\")\n",
    "\n",
    "# display dataframe\n",
    "display(df)\n",
    "\n",
    "# correct way to handle nested json and array format .option(\"multiline\",\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa15786c-6bf1-43da-b751-bfe1d8fa8d33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform initial data exploration to understand the structure\n",
    "\n",
    "# check the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f21d01f-d804-4254-83b7-25fe0b8b1498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display all column names\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22c134eb-38ce-4542-9525-e98c84054d5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# count rows and columns\n",
    "print(\"Number of rows:\", df.count())\n",
    "print(\"Number of columns:\", len(df.columns))\n",
    "print(\"Columns:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5691d830-1374-4757-965a-7d7d24e2f64e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check for missing or null values and display the result\n",
    "from pyspark.sql.functions import col,sum,when\n",
    "df.select([sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32301d26-1059-4c73-87c6-de7058c59ec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "# Part 2: Flatten and Extract Transit Data\n",
    "\n",
    "Extract and flatten the following information for each shipment:\n",
    "\n",
    "Shipment Identifiers:\n",
    "\n",
    "1. Tracking number (from trackingNumber field)\n",
    "2. Service type (from service.type field)\n",
    "3. Service description (from service.description field)\n",
    "4. Carrier code (from carrierCode field)\n",
    "'''\n",
    "\n",
    "# use explode() function --- it converts each element in the array into a seperate row\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# Flatten the 'trackDetails' array\n",
    "df_flat = df.select(explode(\"trackDetails\").alias(\"details\"))\n",
    "\n",
    "# shipment identifiers fields\n",
    "df_shipments = df_flat.select(\n",
    "    col(\"details.trackingNumber\").alias(\"tracking_number\"),\n",
    "    col(\"details.service.type\").alias(\"service_type\"),\n",
    "    col(\"details.service.description\").alias(\"service_description\"),\n",
    "    col(\"details.carrierCode\").alias(\"carrier_code\")\n",
    ")\n",
    "\n",
    "# display result\n",
    "df_shipments.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b34a38e3-b351-4ae5-a938-1c7abcb04f45",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761328662205}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Weight and Package Information:\n",
    "1. Package weight (value and units)\n",
    "2. Packaging type\n",
    "'''\n",
    "# extracting weight and package information from details\n",
    "\n",
    "df_weight_pkg = df_flat.select(\n",
    "    col(\"details.trackingNumber\").alias(\"tracking_number\"),\n",
    "    col(\"details.packageWeight.value\").alias(\"package_weight_value\"),\n",
    "    col(\"details.packageWeight.units\").alias(\"package_weight_units\"),\n",
    "    col(\"details.packaging.type\").alias(\"packaging_type\")\n",
    ")\n",
    "\n",
    "# display result\n",
    "df_weight_pkg.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5999209-ea8f-42d8-bfc4-cbcf8a823320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Location Information:\n",
    "1. Origin city, state, postal code\n",
    "2. Destination city, state, postal code\n",
    "\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import col, explode, coalesce, lit\n",
    "\n",
    "df_location = df_flat.select(\n",
    "    col(\"details.trackingNumber\").alias(\"tracking_number\"),\n",
    "\n",
    "    # 1. Origin address\n",
    "    coalesce(col(\"details.shipperAddress.city\"), lit(\"Unknown\")).alias(\"origin_city\"),\n",
    "    coalesce(col(\"details.shipperAddress.stateOrProvinceCode\"), lit(\"Unknown\")).alias(\"origin_state\"),\n",
    "    coalesce(col(\"details.shipperAddress.countryCode\"), lit(\"Unknown\")).alias(\"origin_country\"),\n",
    "\n",
    "    # 2. Destination address\n",
    "    coalesce(col(\"details.destinationAddress.city\"), lit(\"Unknown\")).alias(\"destination_city\"),\n",
    "    coalesce(col(\"details.destinationAddress.stateOrProvinceCode\"), lit(\"Unknown\")).alias(\"destination_state\"),\n",
    "    coalesce(col(\"details.destinationAddress.countryCode\"), lit(\"Unknown\")).alias(\"destination_country\")\n",
    ")\n",
    "\n",
    "# display location information\n",
    "df_location.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94f5d03-3a35-423b-9277-94b0b4da0bff",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761334755874}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Transit Events:\n",
    "For each shipment, extract ALL events from the events array:\n",
    "1. Event type (from eventType field - can be any value like IT, AR, DP, PU, OD, DL, etc.)\n",
    "2. Event timestamp (from timestamp field)\n",
    "3. Event description (from eventDescription field)\n",
    "4. Event location city (from address.city field)\n",
    "5. Event location state (from address.stateOrProvinceCode field)\n",
    "6. Event location postal code (from address.postalCode field)\n",
    "7. Arrival location type (from arrivalLocation field - can be any value)\n",
    "'''\n",
    "\n",
    "# flatten events array in each shipment\n",
    "df_events_flat = df_flat.select(\n",
    "    col(\"details.trackingNumber\").alias(\"tracking_number\"),\n",
    "    explode(\"details.events\").alias(\"event\")\n",
    ")\n",
    "\n",
    "# extracting the event fields\n",
    "\n",
    "df_events = df_events_flat.select(\n",
    "    col(\"tracking_number\"),\n",
    "    col(\"event.eventType\").alias(\"event_type\"),\n",
    "    col(\"event.timestamp\").alias(\"event_timestamp\"),\n",
    "    col(\"event.eventDescription\").alias(\"event_description\"),\n",
    "    col(\"event.address.city\").alias(\"event_city\"),\n",
    "    col(\"event.address.stateOrProvinceCode\").alias(\"event_state\"),\n",
    "    col(\"event.address.postalCode\").alias(\"event_postal_code\"),\n",
    "    col(\"event.arrivalLocation\").alias(\"arrival_location_type\")\n",
    ")\n",
    "\n",
    "# display transit events\n",
    "df_events.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1737e495-a432-40c0-8107-6dbdd32271fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Part 3: Compute Transit Performance Metrics\n",
    "\n",
    "For each shipment, calculate the following derived metrics:\n",
    "\n",
    "1. Facility Touchpoints:\n",
    "\n",
    "i. Total number of unique facilities visited (count distinct facilities from events where\n",
    "arrivalLocation field contains the substring \"FACILITY\")\n",
    "\n",
    "ii. Number of events with specific event types (count events by eventType - you should\n",
    "identify which event types represent \"in transit\" vs \"arrival\" based on the data)\n",
    "\n",
    "iii. List all unique event types found in the dataset\n",
    "\n",
    "'''\n",
    "\n",
    "# we define facility touchpoints as events where arrival_location_type contains \"FACILITY\"\n",
    "from pyspark.sql.functions import col, when, countDistinct\n",
    "\n",
    "# Filter events for facility touchpoints\n",
    "df_facilities = df_events.filter(col(\"arrival_location_type\").contains(\"FACILITY\"))\n",
    "\n",
    "# Count unique facilities per shipment\n",
    "df_facility_count = df_facilities.groupBy(\"tracking_number\") \\\n",
    "    .agg(countDistinct(\"arrival_location_type\").alias(\"unique_facilities_count\"))\n",
    "\n",
    "# i. Total number of unique facilities visited\n",
    "# display Total number of unique facilities \n",
    "df_facility_count.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "800ff587-924b-4ccf-8487-95682d1a68bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# count events by event type\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Count number of events by event type per shipment\n",
    "df_event_type_counts = df_events.groupBy(\"tracking_number\", \"event_type\") \\\n",
    "    .agg(count(\"*\").alias(\"event_count\")) \\\n",
    "    .orderBy(\"tracking_number\")\n",
    "\n",
    "# display the number of event count\n",
    "df_event_type_counts.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c46959e-d97d-4360-adaa-c46be920fba3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define event type categories\n",
    "# ii. Number of events with specific event types\n",
    "in_transit_types = [\"IT\", \"DP\", \"OD\"]\n",
    "arrival_types = [\"AR\", \"PU\", \"DL\"]\n",
    "\n",
    "df_events_categorized = df_events.withColumn(\n",
    "    \"event_category\",\n",
    "    when(col(\"event_type\").isin(in_transit_types), \"IN_TRANSIT\")\n",
    "    .when(col(\"event_type\").isin(arrival_types), \"ARRIVAL\")\n",
    "    .otherwise(\"OTHER\")\n",
    ")\n",
    "\n",
    "# Count per category per shipment\n",
    "df_event_category_count = df_events_categorized.groupBy(\"tracking_number\", \"event_category\") \\\n",
    "    .agg(count(\"*\").alias(\"count_events\"))\n",
    "\n",
    "# display Number of events with specific event types\n",
    "df_event_category_count.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b679292-4b4f-4a18-a94f-2c446abc58ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# iii. list all unique types in dataset and display the result\n",
    "df_events.select(\"event_type\").distinct().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf6e2338-41e3-42e2-98d8-c42f1489727b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example join\n",
    "df_summary = df_facility_count.join(df_event_category_count.groupBy(\"tracking_number\") \\\n",
    "    .pivot(\"event_category\").sum(\"count_events\"), on=\"tracking_number\", how=\"left\")\n",
    "\n",
    "display(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6376757e-0bc6-419a-af36-6a517cd9dcf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "2. Transit Time Analysis:\n",
    "\n",
    "i. Total transit time in hours (from first pickup-type event to final delivery-type event -\n",
    "identify these by analyzing eventType and eventDescription fields)\n",
    "\n",
    "ii. Time in inter-facility transit in hours (calculate based on event timestamps and facility\n",
    "touchpoints)\n",
    "'''\n",
    "\n",
    "# Extract Epoch Milliseconds and Convert to Timestamp\n",
    "\n",
    "# from pyspark.sql.functions import col, (unix_timestamp, from_unixtime)\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Extract the number inside the struct and convert to long\n",
    "df_events_ts = df_events.withColumn(\n",
    "    \"event_epoch_ms\",\n",
    "    col(\"event_timestamp.$numberLong\").cast(LongType())\n",
    ")\n",
    "\n",
    "# Convert milliseconds to seconds\n",
    "df_events_ts = df_events_ts.withColumn(\n",
    "    \"event_timestamp_ts\",\n",
    "    (col(\"event_epoch_ms\") / 1000).cast(\"timestamp\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1efe8955-0844-48a7-b14d-156869347ea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# identify pickup and delivery events\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Create flags for pickup and delivery events\n",
    "df_events_flagged = df_events_ts.withColumn(\n",
    "    \"is_pickup\",\n",
    "    when(col(\"event_type\") == \"PU\", 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"is_delivery\",\n",
    "    when(col(\"event_type\") == \"DL\", 1).otherwise(0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "990d3304-b046-4fd4-90ec-af330e304f13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# calculate total transit time per shipment\n",
    "from pyspark.sql.functions import min, max, unix_timestamp\n",
    "\n",
    "df_transit_time = df_events_flagged.groupBy(\"tracking_number\").agg(\n",
    "    min(when(col(\"is_pickup\") == 1, col(\"event_timestamp_ts\"))).alias(\"pickup_time\"),\n",
    "    max(when(col(\"is_delivery\") == 1, col(\"event_timestamp_ts\"))).alias(\"delivery_time\")\n",
    ")\n",
    "\n",
    "# i. Calculating transit time in hours (from first pickup-type event to final delivery-type event -\n",
    "# identify these by analyzing eventType and eventDescription fields)\n",
    "df_transit_time = df_transit_time.withColumn(\n",
    "    \"total_transit_hours\",\n",
    "    (unix_timestamp(col(\"delivery_time\")) - unix_timestamp(col(\"pickup_time\"))) / 3600\n",
    ")\n",
    "\n",
    "# display the transit time in hours\n",
    "df_transit_time.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "befa0196-54fa-495a-bb86-33afab0ddf54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ii. Time in inter-facility transit in hours (calculate based on event timestamps and facility\n",
    "touchpoints) \n",
    "\n",
    "'''\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "# Filter facility events\n",
    "df_facility_events = df_events_flagged.filter(col(\"arrival_location_type\").contains(\"FACILITY\"))\n",
    "\n",
    "# Define window partitioned by shipment, ordered by timestamp\n",
    "window_spec = Window.partitionBy(\"tracking_number\").orderBy(\"event_timestamp_ts\")\n",
    "\n",
    "# Get previous facility timestamp\n",
    "df_facility_events = df_facility_events.withColumn(\n",
    "    \"prev_facility_ts\",\n",
    "    lag(\"event_timestamp_ts\").over(window_spec)\n",
    ")\n",
    "\n",
    "# Calculate inter-facility time in hours\n",
    "df_facility_events = df_facility_events.withColumn(\n",
    "    \"inter_facility_hours\",\n",
    "    (unix_timestamp(col(\"event_timestamp_ts\")) - unix_timestamp(col(\"prev_facility_ts\"))) / 3600\n",
    ")\n",
    "\n",
    "# Total inter-facility transit time per shipment\n",
    "df_inter_facility_time = df_facility_events.groupBy(\"tracking_number\").agg(\n",
    "    sum(\"inter_facility_hours\").alias(\"total_inter_facility_hours\")\n",
    ")\n",
    "\n",
    "# display the totalinterfacility time in hours\n",
    "df_inter_facility_time.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27b2061d-ea43-4405-8836-6b347cf80dc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "3. Transit Velocity:\n",
    "i. Average hours per facility (total transit time / number of facilities)\n",
    "ii. Service category classification (classify based on service.type field - you should\n",
    "identify patterns like express vs standard services)\n",
    "\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import col, when, min, max, unix_timestamp, countDistinct\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Convert event_timestamp (struct with $numberLong) to proper timestamp\n",
    "df_events_ts = df_events.withColumn(\n",
    "    \"event_epoch_ms\",\n",
    "    col(\"event_timestamp.$numberLong\").cast(LongType())\n",
    ").withColumn(\n",
    "    \"event_timestamp_ts\",\n",
    "    (col(\"event_epoch_ms\") / 1000).cast(\"timestamp\")\n",
    ")\n",
    "\n",
    "# Flag pickup and delivery events\n",
    "df_events_flagged = df_events_ts.withColumn(\n",
    "    \"is_pickup\",\n",
    "    when(col(\"event_type\") == \"PU\", 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"is_delivery\",\n",
    "    when(col(\"event_type\") == \"DL\", 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Calculate total transit time per shipment\n",
    "df_transit_time = df_events_flagged.groupBy(\"tracking_number\").agg(\n",
    "    min(when(col(\"is_pickup\") == 1, col(\"event_timestamp_ts\"))).alias(\"pickup_time\"),\n",
    "    max(when(col(\"is_delivery\") == 1, col(\"event_timestamp_ts\"))).alias(\"delivery_time\")\n",
    ").withColumn(\n",
    "    \"total_transit_hours\",\n",
    "    (unix_timestamp(col(\"delivery_time\")) - unix_timestamp(col(\"pickup_time\"))) / 3600\n",
    ")\n",
    "\n",
    "# Count unique facility touchpoints\n",
    "df_facilities = df_events_flagged.filter(col(\"arrival_location_type\").contains(\"FACILITY\"))\n",
    "df_facility_count = df_facilities.groupBy(\"tracking_number\").agg(\n",
    "    countDistinct(\"arrival_location_type\").alias(\"unique_facilities_count\")\n",
    ")\n",
    "\n",
    "# Combine transit time and facility counts\n",
    "df_transit_summary = df_transit_time.join(df_facility_count, on=\"tracking_number\", how=\"left\")\n",
    "\n",
    "# display result\n",
    "df_transit_summary.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a4a9649-2726-423c-a524-af417222e628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# i. calculating avg hours per facility (total transit time / number of facilities)\n",
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "# Ensure unique_facilities_count is > 0 to avoid division by zero\n",
    "df_transit_velocity = df_transit_summary.withColumn(\n",
    "    \"avg_hours_per_facility\",\n",
    "    round(col(\"total_transit_hours\") / col(\"unique_facilities_count\"), 2)\n",
    ")\n",
    "\n",
    "# display avg hours per facility\n",
    "df_transit_velocity.select(\"tracking_number\", \"total_transit_hours\", \"unique_facilities_count\", \"avg_hours_per_facility\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0efd0ec4-7f4a-439b-99e1-be78c31b3903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ii. Service category classification (classify based on service.type field - you should\n",
    "identify patterns like express vs standard services)\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Join service info with transit summary\n",
    "df_velocity_service = df_transit_velocity.join(\n",
    "    df_shipments.select(\"tracking_number\", \"service_type\"),\n",
    "    on=\"tracking_number\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Classify service category\n",
    "df_velocity_service = df_velocity_service.withColumn(\n",
    "    \"service_category\",\n",
    "    when(col(\"service_type\").rlike(\"(?i)EXPRESS|OVERNIGHT|PRIORITY\"), \"EXPRESS\")\n",
    "    .when(col(\"service_type\").rlike(\"(?i)GROUND|STANDARD\"), \"STANDARD\")\n",
    "    .otherwise(\"OTHER\")\n",
    ")\n",
    "\n",
    "# display service category\n",
    "df_velocity_service.select(\n",
    "    \"tracking_number\", \"service_type\", \"service_category\", \"avg_hours_per_facility\"\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc6292c4-713b-4def-ab00-f3f745bc1844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "4. Delivery Characteristics:\n",
    "i. Final delivery location type (from deliveryLocationType field)\n",
    "ii. Number of out-for-delivery attempts (count events where eventType or\n",
    "eventDescription indicates out-for-delivery status)\n",
    "iii. Was delivered on first attempt (TRUE if only 1 out-for-delivery event, FALSE otherwise)\n",
    "\n",
    "'''\n",
    "\n",
    "# i. Final delivery location type\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Extract deliveryLocationType from trackDetails\n",
    "df_delivery_type = df.selectExpr(\"explode(trackDetails) as details\") \\\n",
    "    .select(\n",
    "        col(\"details.trackingNumber\").alias(\"tracking_number\"),\n",
    "        col(\"details.deliveryLocationType\").alias(\"delivery_location_type\")\n",
    "    )\n",
    "\n",
    "# display delivery location type\n",
    "df_delivery_type.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1aaaa3c-914a-4d49-94ae-bebc1e211fc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ii. Number of out-for-delivery attempts\n",
    "from pyspark.sql.functions import when, sum as spark_sum, lower\n",
    "\n",
    "# Flag out-for-delivery events\n",
    "df_out_for_delivery = df_events.withColumn(\n",
    "    \"is_out_for_delivery\",\n",
    "    when(\n",
    "        (col(\"event_type\") == \"OD\") |\n",
    "        (lower(col(\"event_description\")).contains(\"out-for-delivery\")),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Count out-for-delivery events per shipment\n",
    "df_delivery_attempts = df_out_for_delivery.groupBy(\"tracking_number\").agg(\n",
    "    spark_sum(\"is_out_for_delivery\").alias(\"num_out_for_delivery_attempts\")\n",
    ")\n",
    "\n",
    "# display number of out-for-delivery attempts\n",
    "df_delivery_attempts.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbb7c3d5-b283-42b0-a656-cd67ecd7c4ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# iii. Was delivered on first attempt\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_delivery_attempts = df_delivery_attempts.withColumn(\n",
    "    \"delivered_first_attempt\",\n",
    "    when(col(\"num_out_for_delivery_attempts\") == 1, True).otherwise(False)\n",
    ")\n",
    "\n",
    "# display delivered_first_attempt\n",
    "df_delivery_attempts.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13befa4a-bc89-4ea2-8937-ed046a5827ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Combine with Delivery Location Type\n",
    "df_delivery_characteristics = df_delivery_type.join(\n",
    "    df_delivery_attempts,\n",
    "    on=\"tracking_number\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# combine and display the results\n",
    "\n",
    "df_delivery_characteristics.select(\n",
    "    \"tracking_number\",\n",
    "    \"delivery_location_type\",\n",
    "    \"num_out_for_delivery_attempts\",\n",
    "    \"delivered_first_attempt\"\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b55c5850-5182-4270-a60b-5b00e05a9799",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Part 4: Handle Edge Cases\n",
    "Your solution should handle:\n",
    "\n",
    "i. Shipments with missing or null values in any field\n",
    "ii. Timestamps in different formats (MongoDB $numberLong format vs ISO string format)\n",
    "iii. Shipments with incomplete event sequences\n",
    "iv. Missing address information (city, state, postal code)\n",
    "v. Duplicate events at the same timestamp\n",
    "vi. Events array being empty or missing\n",
    "vii. Nested fields that may not exist in all records\n",
    "\n",
    "'''\n",
    "\n",
    "# i. Shipments with missing or null values in any field\n",
    "from pyspark.sql.functions import coalesce, lit\n",
    "\n",
    "df_shipments_clean = df.selectExpr(\"explode(trackDetails) as details\").select(\n",
    "    col(\"details.trackingNumber\").alias(\"tracking_number\"),\n",
    "    coalesce(col(\"details.service.type\"), lit(\"Unknown\")).alias(\"service_type\"),\n",
    "    coalesce(col(\"details.service.description\"), lit(\"Unknown\")).alias(\"service_description\"),\n",
    "    coalesce(col(\"details.carrierCode\"), lit(\"Unknown\")).alias(\"carrier_code\"),\n",
    "    coalesce(col(\"details.packageWeight.value\"), lit(0.0)).alias(\"package_weight_value\"),\n",
    "    coalesce(col(\"details.packageWeight.units\"), lit(\"Unknown\")).alias(\"package_weight_units\"),\n",
    "    coalesce(col(\"details.packaging\"), lit(\"Unknown\")).alias(\"packaging_type\"),\n",
    "    coalesce(col(\"details.deliveryLocationType\"), lit(\"Unknown\")).alias(\"delivery_location_type\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17001d25-1156-4d82-acd5-7e2e7765e73f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ii. Timestamps in different formats\n",
    "from pyspark.sql.functions import to_timestamp, when, col\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "df_events_ts = df_events.withColumn(\n",
    "    \"event_timestamp_ts\",\n",
    "    when(col(\"event_timestamp.$numberLong\").isNotNull(),\n",
    "         (col(\"event_timestamp.$numberLong\").cast(LongType()) / 1000).cast(\"timestamp\")\n",
    "    ).otherwise(\n",
    "         to_timestamp(col(\"event_timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd9bd221-4f81-4ce7-9ebb-373bcd94fee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# iii. Shipments with Incomplete Event Sequences\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "df_transit_time = df_events_ts.groupBy(\"tracking_number\").agg(\n",
    "    min(when(col(\"event_type\") == \"PU\", col(\"event_timestamp_ts\"))).alias(\"pickup_time\"),\n",
    "    max(when(col(\"event_type\") == \"DL\", col(\"event_timestamp_ts\"))).alias(\"delivery_time\")\n",
    ").withColumn(\n",
    "    \"total_transit_hours\",\n",
    "    when(\n",
    "        col(\"pickup_time\").isNotNull() & col(\"delivery_time\").isNotNull(),\n",
    "        (unix_timestamp(col(\"delivery_time\")) - unix_timestamp(col(\"pickup_time\"))) / 3600\n",
    "    ).otherwise(None)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c15cb5d-5f93-4fad-b72b-ea61dbee97ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# iv. Missing address information (city, state, postal code)\n",
    "\n",
    "df_location_clean = df_flat.select(\n",
    "    col(\"details.trackingNumber\").alias(\"tracking_number\"),\n",
    "    coalesce(col(\"details.shipperAddress.city\"), lit(\"Unknown\")).alias(\"origin_city\"),\n",
    "    coalesce(col(\"details.shipperAddress.stateOrProvinceCode\"), lit(\"Unknown\")).alias(\"origin_state\"),\n",
    "    coalesce(col(\"details.shipperAddress.countryCode\"), lit(\"Unknown\")).alias(\"origin_country\"),\n",
    "    coalesce(col(\"details.destinationAddress.city\"), lit(\"Unknown\")).alias(\"destination_city\"),\n",
    "    coalesce(col(\"details.destinationAddress.stateOrProvinceCode\"), lit(\"Unknown\")).alias(\"destination_state\"),\n",
    "    coalesce(col(\"details.destinationAddress.countryCode\"), lit(\"Unknown\")).alias(\"destination_country\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee855581-c8d9-4043-be2f-041f7b625f4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# v. Duplicate events at the same timestamp\n",
    "df_events_nodup = df_events_ts.dropDuplicates([\"tracking_number\", \"event_timestamp_ts\", \"event_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9dd2ef3-7617-4eab-bbdf-eea87afe1869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# vi. Events array being empty or missing\n",
    "\n",
    "from pyspark.sql.functions import explode_outer\n",
    "\n",
    "df_flat_events = df.select(explode_outer(\"trackDetails\").alias(\"details\")) \\\n",
    "                   .select(\"details.trackingNumber\", explode_outer(\"details.events\").alias(\"event\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b93a3f53-c798-4a4c-a505-2263c7e1a883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# vii. Nested fields that may not exist in all records\n",
    "\n",
    "from pyspark.sql.functions import col, lit, coalesce\n",
    "\n",
    "col(\"event.address.city\")  # If missing, coalesce with default\n",
    "coalesce(col(\"event.address.city\"), lit(\"Unknown\")).alias(\"event_city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "250d52fd-a78f-4906-a621-53f725a9ee40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Part 5: Output Detailed Transit CSV\n",
    "Create a CSV file named transit_performance_detailed.csv with the following\n",
    "columns:\n",
    "1. tracking_number\n",
    "2. service_type\n",
    "3. carrier_code\n",
    "4. package_weight_kg\n",
    "5. packaging_type\n",
    "6. origin_city, origin_state, origin_pincode\n",
    "7. destination_city, destination_state, destination_pincode\n",
    "8. pickup_datetime_ist, delivery_datetime_ist\n",
    "9. total_transit_hours\n",
    "10. num_facilities_visited\n",
    "11. num_in_transit_events\n",
    "12. time_in_inter_facility_transit_hours\n",
    "13. avg_hours_per_facility\n",
    "14. is_express_service\n",
    "15. delivery_location_type\n",
    "16. num_out_for_delivery_attempts\n",
    "17. first_attempt_delivery\n",
    "18. total_events_count\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_transit_summary = df_transit_summary.join(\n",
    "    df_flat.select(\"tracking_number\",\"package_weight_value\",\"package_weight_units\",\"packaging_type\").distinct(),\n",
    "    \"tracking_number\",\"left\"\n",
    ").withColumn(\n",
    "    \"package_weight_kg\",\n",
    "    when(col(\"package_weight_units\").isin(\"KG\",\"kg\"), col(\"package_weight_value\"))\n",
    "    .when(col(\"package_weight_units\").isin(\"G\",\"g\"), col(\"package_weight_value\")/1000)\n",
    "    .when(col(\"package_weight_units\").isin(\"LB\",\"lb\"), col(\"package_weight_value\")*0.453592)\n",
    "    .otherwise(col(\"package_weight_value\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25dfb059-a64c-47c5-bcce-6f9c4a88c4f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, when, to_timestamp, coalesce, lit, round, countDistinct, unix_timestamp,\n",
    "    min, max, sum as spark_sum\n",
    ")\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "# Fix timestamps safely for mixed formats\n",
    "df_events_ts = df_events.withColumn(\n",
    "    \"event_timestamp_ts\",\n",
    "    when(\n",
    "        col(\"event_timestamp.$numberLong\").isNotNull(),\n",
    "        (col(\"event_timestamp.$numberLong\").cast(LongType()) / 1000).cast(\"timestamp\")\n",
    "    ).otherwise(\n",
    "        to_timestamp(col(\"event_timestamp\").cast(\"string\"), \"yyyy-MM-dd'T'HH:mm:ss\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Flag pickup and delivery events\n",
    "df_events_flagged = df_events_ts.withColumn(\n",
    "    \"is_pickup\", when(col(\"event_type\") == \"PU\", 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"is_delivery\", when(col(\"event_type\") == \"DL\", 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Calculate total transit time correctly using aggregation\n",
    "df_transit_time = df_events_flagged.groupBy(\"tracking_number\").agg(\n",
    "    min(when(col(\"is_pickup\") == 1, col(\"event_timestamp_ts\"))).alias(\"pickup_time\"),\n",
    "    max(when(col(\"is_delivery\") == 1, col(\"event_timestamp_ts\"))).alias(\"delivery_time\")\n",
    ")\n",
    "\n",
    "df_transit_time = df_transit_time.withColumn(\n",
    "    \"total_transit_hours\",\n",
    "    (unix_timestamp(col(\"delivery_time\")) - unix_timestamp(col(\"pickup_time\"))) / 3600\n",
    ")\n",
    "\n",
    "# Inter-facility transit calculation\n",
    "df_facility_events = df_events_flagged.filter(col(\"arrival_location_type\").contains(\"FACILITY\"))\n",
    "window_spec = Window.partitionBy(\"tracking_number\").orderBy(\"event_timestamp_ts\")\n",
    "\n",
    "df_facility_events = df_facility_events.withColumn(\n",
    "    \"prev_facility_ts\", lag(\"event_timestamp_ts\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"inter_facility_hours\",\n",
    "    (unix_timestamp(col(\"event_timestamp_ts\")) - unix_timestamp(col(\"prev_facility_ts\"))) / 3600\n",
    ")\n",
    "\n",
    "df_inter_facility_time = df_facility_events.groupBy(\"tracking_number\").agg(\n",
    "    spark_sum(\"inter_facility_hours\").alias(\"total_inter_facility_hours\")\n",
    ")\n",
    "\n",
    "# Join all dataframes together\n",
    "df_final = (\n",
    "    df_shipments\n",
    "    .join(df_weight_pkg, \"tracking_number\", \"left\")\n",
    "    .join(df_location, \"tracking_number\", \"left\")\n",
    "    .join(df_transit_time, \"tracking_number\", \"left\")\n",
    "    .join(df_facility_count, \"tracking_number\", \"left\")\n",
    "    .join(df_event_category_count.groupBy(\"tracking_number\")\n",
    "           .pivot(\"event_category\").sum(\"count_events\"), \"tracking_number\", \"left\")\n",
    "    .join(df_inter_facility_time, \"tracking_number\", \"left\")\n",
    "    .join(df_velocity_service.select(\"tracking_number\", \"avg_hours_per_facility\", \"service_category\"), \"tracking_number\", \"left\")\n",
    "    .join(df_delivery_characteristics, \"tracking_number\", \"left\")\n",
    ")\n",
    "\n",
    "# Select and rename columns for final CSV\n",
    "df_final = df_final.select(\n",
    "    col(\"tracking_number\"),\n",
    "    col(\"service_type\"),\n",
    "    col(\"carrier_code\"),\n",
    "    round(col(\"package_weight_value\"), 2).alias(\"package_weight_kg\"),\n",
    "    col(\"packaging_type\"),\n",
    "    col(\"origin_city\"),\n",
    "    col(\"origin_state\"),\n",
    "    col(\"origin_country\").alias(\"origin_pincode\"),  # Replace if postal code exists\n",
    "    col(\"destination_city\"),\n",
    "    col(\"destination_state\"),\n",
    "    col(\"destination_country\").alias(\"destination_pincode\"),  # Replace if postal code exists\n",
    "    col(\"pickup_time\").alias(\"pickup_datetime_ist\"),\n",
    "    col(\"delivery_time\").alias(\"delivery_datetime_ist\"),\n",
    "    round(col(\"total_transit_hours\"), 2).alias(\"total_transit_hours\"),\n",
    "    col(\"unique_facilities_count\").alias(\"num_facilities_visited\"),\n",
    "    coalesce(col(\"IN_TRANSIT\"), lit(0)).alias(\"num_in_transit_events\"),\n",
    "    round(col(\"total_inter_facility_hours\"), 2).alias(\"time_in_inter_facility_transit_hours\"),\n",
    "    round(col(\"avg_hours_per_facility\"), 2).alias(\"avg_hours_per_facility\"),\n",
    "    when(col(\"service_category\") == \"EXPRESS\", lit(True)).otherwise(lit(False)).alias(\"is_express_service\"),\n",
    "    col(\"delivery_location_type\"),\n",
    "    col(\"num_out_for_delivery_attempts\"),\n",
    "    col(\"delivered_first_attempt\").alias(\"first_attempt_delivery\"),\n",
    "    (coalesce(col(\"IN_TRANSIT\"), lit(0)) +\n",
    "     coalesce(col(\"ARRIVAL\"), lit(0)) +\n",
    "     coalesce(col(\"OTHER\"), lit(0))).alias(\"total_events_count\")\n",
    ")\n",
    "\n",
    "# Handle missing values\n",
    "df_final = df_final.fillna({\n",
    "    \"num_facilities_visited\": 0,\n",
    "    \"num_in_transit_events\": 0,\n",
    "    \"time_in_inter_facility_transit_hours\": 0.0,\n",
    "    \"avg_hours_per_facility\": 0.0,\n",
    "    \"total_transit_hours\": 0.0,\n",
    "    \"is_express_service\": False,\n",
    "    \"first_attempt_delivery\": False\n",
    "})\n",
    "\n",
    "# displaying the result\n",
    "df_final.display()\n",
    "\n",
    "\n",
    "# Convert to IST timezone\n",
    "# from pyspark.sql.functions import from_utc_timestamp\n",
    "# df_final = df_final.withColumn(\"pickup_datetime_ist\", from_utc_timestamp(\"pickup_datetime_ist\", \"Asia/Kolkata\"))\n",
    "# df_final = df_final.withColumn(\"delivery_datetime_ist\", from_utc_timestamp(\"delivery_datetime_ist\", \"Asia/Kolkata\"))\n",
    "\n",
    "# print(f\"Final CSV successfully created at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78744978-750b-4292-839a-afa2d4ae1a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_path = \"/Volumes/workspace/default/transittarget/transit_performance_detailed.csv\"\n",
    "df_final.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
    "print(f\" Final CSV successfully created at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8430b78a-9cbb-424a-9b51-c13591918ffb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Part 6: Output Network Performance Summary CSV\n",
    "\n",
    "Create a summary CSV file named transit_performance_summary.csv with the\n",
    "following statistics:\n",
    "\n",
    "Overall Metrics:\n",
    "\n",
    "i. total_shipments_analyzed\n",
    "ii. avg_transit_hours, median_transit_hours, std_dev_transit_hours\n",
    "iii. min_transit_hours, max_transit_hours\n",
    "\n",
    "Facility Metrics:\n",
    "i. avg_facilities_per_shipment, median_facilities_per_shipment\n",
    "ii. mode_facilities_per_shipment\n",
    "iii. avg_hours_per_facility, median_hours_per_facility\n",
    "\n",
    "Service Type Comparison:\n",
    "(Group by unique values found in service.type field)\n",
    "\n",
    "i. avg_transit_hours_by_service_type (for each unique service type)\n",
    "ii. avg_facilities_by_service_type (for each unique service type)\n",
    "iii. count_shipments_by_service_type (for each unique service type)\n",
    "\n",
    "\n",
    "Delivery Performance:\n",
    "i. pct_first_attempt_delivery\n",
    "ii. avg_out_for_delivery_attempts\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import statistics\n",
    "\n",
    "# Overall Metrics\n",
    "overall_metrics = (\n",
    "    df_final.agg(\n",
    "        F.count(\"*\").alias(\"total_shipments_analyzed\"),\n",
    "        F.avg(\"total_transit_hours\").alias(\"avg_transit_hours\"),\n",
    "        F.expr(\"percentile(total_transit_hours, 0.5)\").alias(\"median_transit_hours\"),\n",
    "        F.stddev(\"total_transit_hours\").alias(\"std_dev_transit_hours\"),\n",
    "        F.min(\"total_transit_hours\").alias(\"min_transit_hours\"),\n",
    "        F.max(\"total_transit_hours\").alias(\"max_transit_hours\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# display the overall metrics\n",
    "overall_metrics.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cbd7024-2cfb-42ff-8a3e-ef1e0fbdb7c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#  Facility Metrics:\n",
    "facility_metrics = (\n",
    "    df_final.agg(\n",
    "        F.avg(\"num_facilities_visited\").alias(\"avg_facilities_per_shipment\"),\n",
    "        F.expr(\"percentile(num_facilities_visited, 0.5)\").alias(\"median_facilities_per_shipment\"),\n",
    "        F.avg(\"avg_hours_per_facility\").alias(\"avg_hours_per_facility\"),\n",
    "        F.expr(\"percentile(avg_hours_per_facility, 0.5)\").alias(\"median_hours_per_facility\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# displaying the Facility metrics\n",
    "facility_metrics.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a82b54a-94b0-4bf4-b173-a569043d6bcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate mode of facilities manually\n",
    "mode_facility = (\n",
    "    df_final.groupBy(\"num_facilities_visited\")\n",
    "    .count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "    .first()\n",
    ")\n",
    "mode_facilities_per_shipment = mode_facility[\"num_facilities_visited\"] if mode_facility else None\n",
    "\n",
    "facility_metrics = facility_metrics.withColumn(\"mode_facilities_per_shipment\", F.lit(mode_facilities_per_shipment))\n",
    "facility_metrics.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a94aef0-3133-4a32-bfca-2047e423d7be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Service Type Comparison\n",
    "service_type_summary = (\n",
    "    df_final.groupBy(\"service_type\").agg(\n",
    "        F.avg(\"total_transit_hours\").alias(\"avg_transit_hours_by_service_type\"),\n",
    "        F.avg(\"num_facilities_visited\").alias(\"avg_facilities_by_service_type\"),\n",
    "        F.count(\"*\").alias(\"c(ount_shipments_by_service_type\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# displaying the service type comparison\n",
    "service_type_summary.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf86486-e6cb-466e-bd2a-d860cc5f1d4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delivery Performance\n",
    "delivery_metrics = (\n",
    "    df_final.agg(\n",
    "        (F.sum(F.when(F.col(\"first_attempt_delivery\") == True, 1).otherwise(0)) / F.count(\"*\") * 100)\n",
    "        .alias(\"pct_first_attempt_delivery\"),\n",
    "        F.avg(\"num_out_for_delivery_attempts\").alias(\"avg_out_for_delivery_attempts\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# display the delivery performance\n",
    "delivery_metrics.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b10a104f-b8ee-445a-b678-f45893464503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Add a section label to each DataFrame\n",
    "df_summary_labeled = df_summary.withColumn(\"section\", lit(\"Overall Metrics\"))\n",
    "\n",
    "service_type_summary_labeled = service_type_summary.withColumn(\"section\", lit(\"Service Type Comparison\"))\n",
    "\n",
    "# Align columns (fill missing columns in each df with nulls)\n",
    "# Get all columns\n",
    "all_cols = list(set(df_summary_labeled.columns) | set(service_type_summary_labeled.columns))\n",
    "\n",
    "def add_missing_columns(df, all_cols):\n",
    "    for c in all_cols:\n",
    "        if c not in df.columns:\n",
    "            df = df.withColumn(c, lit(None))\n",
    "    return df.select(all_cols)\n",
    "\n",
    "df_summary_aligned = add_missing_columns(df_summary_labeled, all_cols)\n",
    "service_type_aligned = add_missing_columns(service_type_summary_labeled, all_cols)\n",
    "\n",
    "# Union both DataFrames\n",
    "df_combined = df_summary_aligned.unionByName(service_type_aligned)\n",
    "df_combined.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c74a620-83d1-48a6-b35a-40d14152ae83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save combined CSV\n",
    "output_path_combined = \"/Volumes/workspace/default/transitperformance/transit_performance_summary.csv\"\n",
    "df_combined.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path_combined)\n",
    "\n",
    "print(f\"Combined network performance summary CSV created at: {output_path_combined}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Datapipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
